
import newclay.common.*;
import newclay.diagnostics.*;
import maybe.*;
import parsing.combinators.wrapper.*;
import externals.*;
import strings.encodings.utf8.(UTF8Decoder);



//
// data definitions
//

enum TokenKind {
    SENTINEL,
    PUNCTUATION,
    KEYWORD,
    IDENTIFIER,
    STRING_LITERAL,
    CHAR_LITERAL,
    INT_LITERAL,
    FLOAT_LITERAL,
    STATIC_INDEX,
    LITERAL_CODE,
    SPACE,
    COMMENT,
}

alias DataRange = Range[SizeT];

record Token(kind:TokenKind, subKind:UInt8, range:DataRange, indexRange:DataRange);

[I | Integer?(I)]
overload Token(kind:TokenKind, subKind:I) =
    Token(kind, UInt8(subKind),
        DataRange(SizeT(0), SizeT(0)),
        DataRange(SizeT(0), SizeT(0))
    );

overload Token(kind:TokenKind) = Token(kind, 0);

record SourceTokens = referenceType(
    file: SourceFile,
    vector: Vector[Token],
);



//
// punctuations and keywords
//

// 34 punctuations
var punctuations = [
    "<--", "-->",
    "=>", "->",
    "<=", ">=", "<", ">",
    "!=", "==",
    "+=", "-=", "*=", "/=",
    "..",
    ":",
    "(", ")", "[", "]",
    "|",
    "=",
    "'",
    ";", ",",
    "*", ".",
    "{", "}",
    "+", "-",
    "/",
    "&", "^",
    "#",
    "@",
];

// 39 keywords
var keywords = [
    "import", "as", "with",
    "external",
    "inline", "noreturn", "nothrow",
    "define", "extend",
    "overload", "static",
    "var", "const", "ref", "rvalue", "forward",
    "alias",
    "true", "false",
    "if", "else",
    "and", "or", "not",
    "public", "private",
    "return", "break", "continue",
    "try", "catch", "throw",
    "for", "in",
    "while",
    "switch", "case",
    "div", "mod",
    "__c__", "__llvm__", "__asm__",
    "__unreachable__",
];

punctuationIndex(s) Int {
    for (i, x in enumerated(punctuations)) {
        if (x == s)
            return Int(i);
    }
    return -1;
}

keywordIndex(s) Int {
    for (i, x in enumerated(keywords)) {
        if (x == s)
            return Int(i);
    }
    return -1;
}



//
// lexer input
//

record LexerInput(
    file : SourceFile,
    data : UTF8Decoder[CoordinateRange[Pointer[Char]]],
    current : SizeT,
    maxCurrent : SizeT,
);

overload LexerInput(file : SourceFile) =
    LexerInput(file, utf8Iterator(file.data), SizeT(0), SizeT(0));

overload iterator(x:LexerInput) = x;

overload hasNext?(x:LexerInput) = hasNext?(x.data);

overload next(x:LexerInput) {
    var c = next(x.data);
    x.current += 1;
    return c;
}

overload assign(dest:LexerInput, lvalue src:LexerInput) {
    if (dest.file == src.file) {
        var destMax = max(dest.maxCurrent, dest.current);
        var srcMax = max(src.maxCurrent, src.current);
        dest.maxCurrent = max(destMax, srcMax);
        dest.current = src.current;
        dest.data = src.data;
    }
    else {
        dest.file = src.file;
        dest.data = src.data;
        dest.current = src.current;
        dest.maxCurrent = src.maxCurrent;
    }
}

currentByte(x:LexerInput) = SizeT(x.data.iter.begin - begin(x.file.data));


//
// LexerError
//

record LexerError(
    file: SourceFile,
    where: SizeT
);

instance ClayError = LexerError;

overload displayError(e:LexerError) {
    errorWithLocation(e.file, e.where, "invalid token");
}



//
// tokenize
//

external lexer_tokenize(
    ex:ExternalException,
    inFile:Pointer[SourceFile],
    out:Pointer[SourceTokens]
);

tokenize(file) = callExternal(lexer_tokenize, file);
